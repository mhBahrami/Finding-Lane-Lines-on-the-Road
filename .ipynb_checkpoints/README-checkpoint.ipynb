{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finding Lane Lines on the Road** \n",
    "\n",
    "---\n",
    "\n",
    "**Finding Lane Lines on the Road**\n",
    "\n",
    "This project helps to detect lane lines in images using Python and OpenCV and then applying it for videos as an end goal.\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "  1. [Libraries and Packages](#1-libraries-and-packages)\n",
    "  2. [Finding Lane Lines](#2-finding-lane-lines)\n",
    "     - [Modifying `draw_lines()` Function](#modifying-draw_lines-function)\n",
    "  3. [Potential Shortcomings with Current Pipeline](#3-potential-shortcomings-with-current-pipeline)\n",
    "  4. [Possible Improvements to Pipeline](#4-possible-improvements-to-pipeline)\n",
    "  5. [Conclusion](#5-conclusion)\n",
    "  6. [License](#6-license)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Libraries and Packages\n",
    "The following libraries and packages have been used in this project: **`scipy`**, **`matplotlib.pyplot`**, **`matplotlib.image`**, **`numpy`**, **`cv2`**, **`sklearn.linear_model`**, **`sklearn.preprocessing`**, **`sklearn.pipeline`**, **`moviepy.editor`**, and **`IPython.display`**.\n",
    "\n",
    "### 2. Finding Lane Lines\n",
    "\n",
    "First of all I started using the simple methods for making a pipeline, as described in the course. The steps were as following:\n",
    "  \n",
    "  1. Converting the RGB image to a Grayscale image using `cv2.cvtColor()`.\n",
    "  2. Applying `cv2.GaussianBlur()` to make the image smoother.\n",
    "  3. Applying the OpenCV Canny edge detection function, `cv2.Canny()`, to find edges in the image\n",
    "  4. Creating and applying a mask to find the region of interest. The region of interest includs the lane line which the car is moving on. \n",
    "     - It helps to remove other objects on the image that are redundant and make it easier to make pipelines.\n",
    "  5. Applying _Hough Transform_ on the edge detected image to find lines.\n",
    "  6. Modifying the `draw_lines()` function to make pipelines on the image. \n",
    "     - I will describe how to modify `draw_lines()` function later.\n",
    "\n",
    "You can change/adjust the parameters of _Canny_ or _Hough Transform_ functions to find pipelines for the test images. But when it comes to the videos, especially the challange video, it's not able to find pipelines for the all frames accurately!\n",
    "\n",
    "Before explaining _the steps to improve lane line detection_ I will describe `draw_lines()` function modification steps.\n",
    "\n",
    "\n",
    "####  Modifying `draw_lines()` Function\n",
    "In order to draw a single line on the left and right lanes, I modified the `draw_lines(img, lines, color=[255, 0, 0], thickness=8)` function by calculating the left line and the right lane slopes. Then I drew a solid line with the calculated slope. To accomplish that the steps are as following:\n",
    "  1. **Calculate the average of positive slopes and negative slopes separately.** In most cases the left line has a negative (-) slope and the right line has a positive (+) slope. So, this step gives us a good estimate of the final line slopes.\n",
    "      ```python\n",
    "     slopes = [((y2-y1)/(x2-x1)) for line in lines for x1,y1,x2,y2 in line]\n",
    "     lane_slopes_right = np.average(np.array([m for m in slopes if m > 0]))\n",
    "     lane_slopes_left = np.average(np.array([m for m in slopes if m < 0]))\n",
    "      ```\n",
    "  2. **Find the left and right lines.** To define a line uniquely, only **line slope** and **a point on the line** are needed. I created a function called `get_line(slope, x0, y0, y_min, y_max)` to define a line with its parameters and make it reusable.\n",
    "        ```python\n",
    "     def get_line(slope, x0, y0, y_min, y_max):\n",
    "            x1 = int(x0 + (y_max - y0) / slope)\n",
    "            x2 = int(x0 + (y_min - y0) / slope)\n",
    "            return x1, y_max, x2, y_min\n",
    "        ```\n",
    "        This function gets the line parameters (`slope`, `x0`, `y0`) and range of line (`y_min`, `y_max`) and returns the _start point_ (`x1, y_max`) and the _end point_ (`x2, y_min`) of the line.\n",
    "  3. **Finding line parameters `slope, x0, y0, y_min, y_max`.** We know the line `slope` from first step. So, it's time to find the rest!\n",
    "  \n",
    "        i. Categorize the left and right lines.\n",
    "        ```python\n",
    "     lines_left = [line for line in lines for x1,y1,x2,y2 in line if ((y2-y1)/(x2-x1)) < 0]\n",
    "     lines_right = [line for line in lines for x1,y1,x2,y2 in line if ((y2-y1)/(x2-x1)) > 0]\n",
    "        ```\n",
    "        ii. Find the upper (a) and lower (b) points of each side. It helps to find `x0, y0, y_min, y_max`.\n",
    "        ```python\n",
    "     # Left side\n",
    "     l_a_dict = {y1:x1 for line in lines_left for x1, y1, x2, y2 in line}\n",
    "     l_a_y = min(l_a_dict, key=l_a_dict.get)\n",
    "     l_a = [l_a_dict[l_a_y], l_a_y]\n",
    "     l_b_dict = {y2:x2 for line in lines_left for x1, y1, x2, y2 in line}\n",
    "     l_b_y = max(l_b_dict, key=l_b_dict.get)\n",
    "     l_b = [l_b_dict[l_b_y], l_b_y]\n",
    "     \n",
    "     # Right side\n",
    "     r_a_dict = {y1:x1 for line in lines_right for x1, y1, x2, y2 in line}\n",
    "     r_a_y = min(r_a_dict, key=r_a_dict.get)\n",
    "     r_a = [r_a_dict[r_a_y], r_a_y]\n",
    "     r_b_dict = {y2:x2 for line in lines_right for x1, y1, x2, y2 in line}\n",
    "     r_b_y = max(r_b_dict, key=r_b_dict.get)\n",
    "     r_b = [r_b_dict[r_b_y], r_b_y]\n",
    "     ```\n",
    "        I used `l_b` and `r_b` as `x0, y0` for the left and the right lines respectively.\n",
    "        \n",
    "      iii. Finding vertical range of lines `y_min, y_max`.\n",
    "        ```python\n",
    "     ymin = min(l_b_y, r_a_y)\n",
    "     ymax = max(l_a_y, r_b_y)\n",
    "        ```\n",
    "  4. **Add a single line to the image as the left pipeline.** I used `cv2.line()` function for adding a line to the image.\n",
    "        ```python\n",
    "        l_x1, l_y1, l_x2, l_y2 = get_line(lane_slopes_left, l_b[0], l_b[1], ymin, ymax)\n",
    "        cv2.line(img, (l_x1, l_y1), (l_x2, l_y2), color, thickness)\n",
    "        ```\n",
    "        Do the same thing for the right side:\n",
    "        ```python\n",
    "        r_x1, r_y1, r_x2, r_y2 = get_line(lane_slopes_right, r_a[0], r_a[1], ymin, ymax)\n",
    "        cv2.line(img, (r_x1, r_y1), (r_x2, r_y2), color, thickness)\n",
    "        ```\n",
    "        `weighted_img()` also could be used instead of `cv2.line()` for adding translucent pipelines to the original image.\n",
    "\n",
    "### 3. Potential Shortcomings with Current Pipeline\n",
    "Before improving the lane line detection code, potential shortcomings need to be identified. There are two important shortcoming here:\n",
    "  1. **Color selection:** The algorithm for color selection is quite simple and it's useless, for instance, when the brightness of the image is high or the contrast is low or there is a light-color object on the road close to the lane line sides, etc.\n",
    "  2. **Finding the solid left and right lines:** The road isn't always straight and it turns to the left an right. So, _striaght pipeline_ is a wrong assumption. The truth is that a pipeline is actually a curve!\n",
    "  \n",
    "### 4. Possible Improvements to Pipeline\n",
    "First step is to _improve the color selection algorithm_ and second step is _estimating the best possible curve line for the pipeline_. For the rest of this section I use `test_images/whiteCarLaneSwitch.jpg` to show result of each step.\n",
    "\n",
    "<br/>\n",
    "<figure>\n",
    "    <img src=\"article/whiteCarLaneSwitch_source.jpg\" width=\"460\" alt=\"Combined Image\" />\n",
    "    <figcaption>\n",
    "        <p></p> \n",
    "        <p style=\"text-align: center;\"> <i> The image which is used to show result of each step. </i> </p> \n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "#### Color Selection\n",
    "A possible improvement would be using a different color space like HLS instead of RGB for selecting white and yellow colors. Another one is that do this separately for each color!\n",
    "\n",
    "##### Different Color Space\n",
    "To start, I created `copy_hls()` function for converting color space from `RGB` to `HLS`. \n",
    "```python\n",
    "def copy_hls(image):\n",
    "    img = np.copy(image)\n",
    "    # Convert BGR to HLS\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    return hls\n",
    "```\n",
    "\n",
    "> **Note:** It is a good practice to initially create a copy of the source image and then make changes to the copied version instead of the main version.\n",
    "\n",
    "##### White and Yellow Mask\n",
    "The  `get_white_yellow_mask(img)` function generates a mask to filter out almost every objects that doesn't have white or yellow colors. Using the lower and upper range of color and `cv2.inRange()` the mask for the a given color could be found. `mask_y` for the yellow objects:\n",
    "```python\n",
    "# Convert BGR to HLS\n",
    "img_hls = copy_hls(img)\n",
    "\n",
    "# Yellow mask\n",
    "# define range of yellow color in HLS color space\n",
    "lower_range = np.array(y_lower, dtype=np.uint8)\n",
    "upper_range = np.array(y_upper, dtype=np.uint8)\n",
    "\n",
    "# Apply the range values to the HLS image to get only yellow colors\n",
    "mask_y = cv2.inRange(img_hls, lower_range, upper_range)\n",
    "```\n",
    "and `mask_w` for the white color objects:\n",
    "```python\n",
    "# White mask\n",
    "# define range of white color in HLS color space\n",
    "lower_range = np.array(w_lower, dtype=np.uint8)\n",
    "upper_range = np.array(w_upper, dtype=np.uint8)\n",
    "\n",
    "# Apply the range values to the HLS image to get only white colors\n",
    "mask_w = cv2.inRange(img_hls, lower_range, upper_range)\n",
    "```\n",
    "> At the end of section 3, I will explain about lower and upper ranges of yellow and white colors.\n",
    "\n",
    "Then mix `mask_y` and `mask_w` to a single mask for both colors using `cv2.bitwise_or()`.\n",
    "```python\n",
    "# Mix the generated masks\n",
    "mask_wy = cv2.bitwise_or(mask_y, mask_w)\n",
    "idx = mask_wy != 0\n",
    "mask_wy[idx] = 255\n",
    "```\n",
    "\n",
    "The result is as following:\n",
    "\n",
    "<table style=\"width:100%; text-align: center;\">\n",
    "  <tr>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_mask_y.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> `mask_y`: Yellow color mask </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_mask_w.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> `mask_w`: White color mask </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td colspan=\"2\">\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_mask_wy.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "            <p></p> \n",
    "            <p style=\"text-align: center;\"> <i> `mask_wy`: Mixed mask for both yellow and white </i> </p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Canny Edge Detection\n",
    "The `get_canny(img, mask)` function creates a convenient canny image. By using `mask` we can extract the regions in the image that includes white and/or yellow objects. I call it `region_of_interest`.\n",
    "```python\n",
    "region_of_interest = cv2.bitwise_and(img, mask)\n",
    "```\n",
    "Then I made it a little bit blur to reduce the noise of the image which also helps gaining a better canny image.\n",
    "```python\n",
    "region_of_interest_blur = gaussian_blur(region_of_interest, kernel_size)\n",
    "```\n",
    "At the end, I converted it to a canny image using `canny` function. It's one of helper functions and it uses \n",
    "```python\n",
    "region_of_interest_canny = canny(region_of_interest_blur, canny_low_threshold, canny_high_threshold)\n",
    "```\n",
    "\n",
    "You can see the results below:\n",
    "\n",
    "<table style=\"width:100%; text-align: center;\">\n",
    "  <tr>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_region_of_interest.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> `region_of_interest` : The region of interest</i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_region_of_interest_blur.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> `region_of_interest_blur`: The blurred region of interest to reduce the noise</i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td colspan=\"2\">\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_region_of_interest_canny.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "            <p></p> \n",
    "            <p style=\"text-align: center;\"> <i>  `region_of_interest_canny`: Canny edge detected image </i> </p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Transforming the Lane Area to a Rectangle\n",
    "To simplify, the lane area in canny image is transformed to a rectangle. For this purpose `transform_to_rectangle()` has been used. In order to implement this function `cv2.getPerspectiveTransform()` and `cv2.warpPerspective()` of OpenCV are used.\n",
    "<br/>\n",
    "<table style=\"width:100%; text-align: center;\">\n",
    "  <tr>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_region_of_interest_canny.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> Canny edge detected image</i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_img_trans_org2rec.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> Transforming the lane line area to the rectangle </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Separating the Left and Right Sides\n",
    "Before finding the left and right curved pipelines, I separated the left and right lines using `separate_to_left_right()` as following:\n",
    "\n",
    "<table style=\"width:100%; text-align: center;\">\n",
    "  <tr>\n",
    "    <td colspan=\"2\">\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_img_trans_org2rec.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "            <p></p> \n",
    "            <p style=\"text-align: center;\"> <i>  Transforming the lane line area to the rectangle </i> </p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_img_left.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> Used for finding the left curved line</i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_img_right.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> Used for finding the right curved line </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Hough Transform\n",
    "Next step is applying _Hough Transform_ by using `find_hough_lines()` function to the both left and right side images. The results for each side is a set of lines and each line contains a couple of points that indicates the starting and ending points of it.\n",
    "You can see these points below:\n",
    "<table style=\"width:100%; text-align: center;\">\n",
    "  <tr>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_img_left_plus_hough_lines.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> The starting and ending points of Hough Transform lines for the left side </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_img_right_plus_hough_lines.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> The starting and ending points of Hough Transform lines for the right side </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Find the Best Line with Polynomial Interpolation\n",
    "`scikit-learn` has some pakcages for Polynomial Interpolation (see an example [here](http://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html)). In fact, I used linear regression with polynomial features to approximate a nonlinear function that fits the points which were found from the previous step. I used the following packages:\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "```\n",
    "The function that is responsible to fit the best line is `find_pipeline`. The polynomial with a degree equal to 2 gives the best results. If you increase or decrease the degree of polynomial, the underfitting and overfitting problems occur. For more information see [here](http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py). Finally, the calculated pipeline formula would be used to draw pipelines using `draw_pred_lines()` function (image below).\n",
    "<br/>\n",
    "<table style=\"width:100%; text-align: center;\">\n",
    "  <tr>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_calculated_curved_pipelines.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> The calculated pipelines </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "> I created a `buffer_model` for the calculated pipeline models. In each step after calculating the model I stored them (left and right models) in this buffer. And they will be used to a more accurate estimation of the pipelines for the next frame. To clarify, assume we have **two first frames** of a video: `F1` and `F2`. The steps are as following:\n",
    ">  1. Calculate the pipeline predict model for `F1` (`pred_model_1`).\n",
    ">  2. Use this `pred_model_1` to add an estimated pipelines to `F1`.\n",
    ">  3. Add it to the buffer.\n",
    ">  4. Next frame (`F2`) comes for estimation.\n",
    ">  5. Calculate the pipeline predict model for `F2`(`pred_model_2`).\n",
    ">  6. Use `pred_model_1` and `pred_model_2` to add an estimated pipelines to `F2`.\n",
    ">     - If deviation of a point on the pipe line 2 with respect to the same point on the line 1 exceeds a specified threshold, `th_0`, replace it with a point which has a deviation equal to the `th_0` otherwise do nothing.\n",
    ">\n",
    "> The step 6 really makes the changes of curvature smooth.\n",
    "\n",
    "#### Adding The Green Zone and Retransform it to the Original Shape\n",
    "The safe driving area for a car is the area between the pipelines that I call it \"Green Zone.\" Let's fill the area between red pipelines with the green color using `add_green_zone()` function. For implementing this function I used `cv2.fillPoly()` from OpenCV (figure below). Then retransform it to the original perspective using `transform_back_to_origin()`. The results are as following:\n",
    "<br/>\n",
    "<table style=\"width:100%; text-align: center;\">\n",
    "  <tr>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_curved_pipelines_with_green_zone.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> The calculated pipelines with green zone</i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_img_trans_rec2org.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> Transformed result </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Adding the Pipelines with Green Zone to the Original Image\n",
    "This is a time for adding the pipeline with green zone to the original image. `weighted_img()` function adds a translucent version to the original image (figure below).\n",
    "<br/>\n",
    "<table style=\"width:100%; text-align: center;\">\n",
    "  <tr>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/whiteCarLaneSwitch_final.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> The final result </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### `find_pipelines_and_green_zone()` Function\n",
    "`find_pipelines_and_green_zone()` function (including all the mentioned steps) adds pipe lines and green zone to a given road image. You can see this function below:\n",
    "```python\n",
    "def find_pipelines_and_green_zone(img):\n",
    "    mask = get_white_yellow_mask(img)\n",
    "    \n",
    "    canny = get_canny(img, mask)\n",
    "\n",
    "    rows,cols,ch = img.shape\n",
    "    img_trans = transform_to_rectangle(canny, rows, cols)\n",
    "    img_left, img_right = separate_to_left_right(img_trans)\n",
    "    \n",
    "    lines_left, lines_right = find_hough_lines(img_left, img_right)\n",
    "    \n",
    "    len_x_history = 40\n",
    "    len_x_plot = 10\n",
    "    x_history = np.int16(np.linspace(0, rows - 1, len_x_history))\n",
    "    x_plot = np.int16(np.linspace(0, rows - 1, len_x_plot)) \n",
    "    y_plot_left, y_plot_right = find_pipeline(rows, cols, lines_left, lines_right, x_history, x_plot)\n",
    "    \n",
    "    # Draw lines\n",
    "    line_width = 40\n",
    "    img_lines = draw_pred_lines(rows, cols, y_plot_left, y_plot_right, x_plot, line_width)\n",
    "    img_green_zone = add_green_zone(img_lines, y_plot_left, y_plot_right, x_plot)\n",
    "    \n",
    "    # Transform region of interest\n",
    "    img_trans_back = transform_back_to_origin(img_green_zone, rows, cols)\n",
    "        \n",
    "    # Add it to the original image\n",
    "    final = weighted_img(img, img_trans_back)\n",
    "    return final\n",
    "```\n",
    "\n",
    "#### Test Images\n",
    "The result of the other test images are as following:\n",
    "\n",
    "<table style=\"width:100%; text-align: center;\">\n",
    "  <tr>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/solidWhiteCurve_final.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> solidWhiteCurve.jpg </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/solidWhiteRight_final.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> solidWhiteRight.jpg </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/solidYellowCurve_final.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> solidYellowCurve.jpg </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/solidYellowCurve2_final.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> solidYellowCurve2.jpg </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/solidYellowCurveChal_final.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> solidYellowCurveChal.jpg </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "    <td>\n",
    "        <figure>\n",
    "            <img src=\"article/solidYellowLeft_final.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    "            <figcaption>\n",
    "                <p></p> \n",
    "                <p style=\"text-align: center;\"> <i> solidYellowLeft.jpg </i></p> \n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Tuning the Parameters\n",
    "I put the parameters for different parts of this code in one place to tune them conveniently. The final result for these parameters are as following:\n",
    "```python\n",
    "# Color in range \n",
    "y_lower = [10, 0, 120]\n",
    "y_upper = [40, 255, 255]\n",
    "w_lower = [16, 182, 0]\n",
    "w_upper = [255, 255, 255]\n",
    "\n",
    "# Transform\n",
    "coef_w_top = 0.15\n",
    "coef_w_dwn = 1.00\n",
    "offset_v_top = 0.63\n",
    "offset_v_dwn = 0.02\n",
    "\n",
    "# Blur\n",
    "kernel_size = 9\n",
    "\n",
    "# Canny\n",
    "canny_low_threshold = 0\n",
    "canny_high_threshold = 255\n",
    "\n",
    "# Make a blank the same size as our image to draw on\n",
    "rho = 1                 # distance resolution in pixels of the Hough grid\n",
    "theta = np.pi/180 * 0.5 # angular resolution in radians of the Hough grid\n",
    "threshold = 10          # minimum number of votes (intersections in Hough grid cell)\n",
    "min_line_len = 15       # minimum number of pixels making up a line\n",
    "max_line_gap = 10       # maximum gap in pixels between connectable line segments\n",
    "\n",
    "# Interpolation\n",
    "degree = 2\n",
    "```\n",
    "\n",
    "These values has been tuned for all videos including the challenge video.\n",
    "\n",
    "##### You can find the output videos [here](test_videos_output) or watch them online [here](https://www.youtube.com/watch?v=S0_758-sbnc&index=2&list=PLgjxKJEo-VjELoTEKXEjA8Vi7s5xZCzI9).\n",
    "\n",
    "\n",
    "### 5. Conclusion\n",
    "Using better approaches for \"color selection\" and \"pipeline estimation\" results to a more accurate output. Also using the buffer makes the changes of the pipeline curvature smoother.\n",
    "\n",
    "### 6. License\n",
    "[MIT License](LICENSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
